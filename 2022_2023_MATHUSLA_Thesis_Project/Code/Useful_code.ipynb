{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook have 3 parts:\n",
    "\n",
    "1. Read one SCV file. \n",
    "2. Process noise from CSV files. Use the code if some rondom noise get coupled in and you are intrested in the noise frequency.\n",
    "3. Process DAQ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = \"DAQ_TAC.csv\" #filename\n",
    "TAC = []\n",
    "with open(filename, newline='') as csvfile:\n",
    "    dataset = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in dataset:\n",
    "        txt = ', '.join(row).split(\",\")\n",
    "        if txt[0] == '':\n",
    "            data_x = float(txt[-1]) #Choose the column you want to read\n",
    "            TAC.append(data_x)\n",
    "TAC = np.array(TAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TAC,range=(5e-3,5e-2),bins = 100)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"TAC amplitude (V)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\"Plot #1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use pyCRP to calculate noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input csv files\n",
    "Take 200 to 400 traces with the oscilloscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrp\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import joblib\n",
    "\n",
    "def process_noise(filename, trace_length = 10000, channel = 1, fs = 1/(2.9992e-06 - 2.999E-06), cut_noise = False):\n",
    "    files = glob.glob(filename)\n",
    "    \n",
    "    noise = [] #empty list for traces\n",
    "\n",
    "    for file in files: # Loop over all the traces\n",
    "        # For one trace\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data_file = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            trace_3 = []\n",
    "            for row in data_file: #loop over the file to pull the traces \n",
    "                txt = ', '.join(row).split(\",\")\n",
    "                if txt[0] == '':\n",
    "                    data_x = txt[3:-1]\n",
    "\n",
    "                    trace_3.append(float(data_x[channel]))\n",
    "\n",
    "\n",
    "            noise.append(trace_3[0:trace_length])\n",
    "    print(\"noise:\",len(noise))\n",
    "    print(\"tracelength:\",len(trace_3))\n",
    "\n",
    "    noise=np.array(noise)\n",
    "\n",
    "    noise = {\"CH1\":noise}\n",
    "\n",
    "    utrace=pycrp.Trace(noise,fs=fs)\n",
    "    if cut_noise:\n",
    "        traces_cut = utrace.autocut_psd(max_iter=8,ds=4,use_filter=\"gauss:30,1\", verbose=True,simultaneous_cut=False)\n",
    "        unoise=pycrp.Noise(traces_cut,fs=fs,trace_weight=1e6,max_traces = 500)\n",
    "    else:\n",
    "        unoise=pycrp.Noise(noise,fs=fs,trace_weight=1e6,max_traces = 500)\n",
    "    unoise.calc_psd()\n",
    "    \n",
    "    #PSDs_with_wire={\"f\":unoise.f,\"PSDs\":unoise.psds}\n",
    "    return {\"f\":unoise.f,\"PSDs\":unoise.psds}\n",
    "\n",
    "def pulse_template(filename, channel = 1,trace_length=2048, fs = 1/(2.9992e-06 - 2.999E-06), pre_trig = 1024, post_trig = 1024):\n",
    "    CH3 = []\n",
    "    trigger_points_CH3 = []\n",
    "    files = glob.glob(filename)\n",
    "    \n",
    "    noise = [] #empty list for traces\n",
    "\n",
    "    for file in files: # Loop over all the traces\n",
    "        # For one trace\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data_file = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            trace_3 = []\n",
    "            for row in data_file: #loop over the file to pull the traces \n",
    "                txt = ', '.join(row).split(\",\")\n",
    "                if txt[0] == '':\n",
    "                    data_x = txt[3:-1]\n",
    "\n",
    "                    trace_3.append(float(data_x[channel]))\n",
    "            trace_3_filtered = scipy.ndimage.gaussian_filter(trace_3,sigma=10, order=0)\n",
    "            trigger_pt_3 = np.argmin(trace_3_filtered) # This is not a rigorous method, but usually enough\n",
    "\n",
    "        #Make sure the trace length is correct, which means the trigger point is not on the edge of the trace\n",
    "        if len(trace_3[(trigger_pt_3-pre_trig):(trigger_pt_3+post_trig)]) == trace_length: \n",
    "            CH3.append(trace_3[(trigger_pt_3-pre_trig):(trigger_pt_3+post_trig)])\n",
    "            trigger_points_CH3.append(trigger_pt_3)\n",
    "    print(\"Number of traces averaged\",len(np.array(CH3)))\n",
    "    \n",
    "    return np.average(np.array(CH3),axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template(pulse average) vs. noise\\\n",
    "Find the cutoff frequency from the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sipm_dark_current_*/*.csv\"\n",
    "pulse_average = pulse_template(filename, channel = 1,trace_length=2048, fs = 1/(2.9992e-06 - 2.999E-06), pre_trig = 1024, post_trig = 1024) \n",
    "# fs is the scope sampling frequency. Find out from the scv file using 1/(t1-t0)\n",
    "mean_0 = pulse_average-np.mean(pulse_average[0:1000])\n",
    "pulse_average_normalize = -mean_0/np.min(mean_0)\n",
    "\n",
    "noise = process_noise(filename, trace_length = 2048, channel = 1, fs = 1/(2.492e-7 - 2.49E-7), cut_noise = True)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(noise[\"f\"]/1e6, noise[\"PSDs\"][\"CH1\"], label='noise',alpha=0.6)\n",
    "freq, power = scipy.signal.periodogram(pulse_average*1e6, fs=1/(2.492e-7 - 2.49E-7))\n",
    "plt.plot(freq/1e6, power, label='pulse',alpha = 0.6)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Frequency (MHz)')\n",
    "plt.ylabel('Power (uV/sqrt(Hz))')\n",
    "plt.ylim(1e-7, 0.5)\n",
    "plt.title(\"Noise PSDs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trace_from_csv(filename, channel = 1,trace_length=2048, fs = 1/(0.02e-7),pre_trig = 1024, post_trig = 1024):\n",
    "    CH3 = []\n",
    "    trigger_points_CH3 = []\n",
    "    files = glob.glob(filename)\n",
    "    \n",
    "    noise = [] #empty list for traces\n",
    "\n",
    "    for file in files: # Loop over all the traces\n",
    "        # For one trace\n",
    "        with open(file, newline='') as csvfile:\n",
    "            data_file = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            trace_3 = []\n",
    "            for row in data_file: #loop over the file to pull the traces \n",
    "                txt = ', '.join(row).split(\",\")\n",
    "                if txt[0] == '':\n",
    "                    data_x = txt[3:-1]\n",
    "\n",
    "                    trace_3.append(float(data_x[channel]))\n",
    "            trace_3_filtered = scipy.ndimage.gaussian_filter(trace_3,sigma=10, order=0)\n",
    "            print(len(trace_3_filtered))\n",
    "            trigger_pt_3 = np.argmin(trace_3_filtered)\n",
    "            #trigger_pt_3 = 1248\n",
    "\n",
    "\n",
    "        if len(trace_3[(trigger_pt_3-pre_trig):(trigger_pt_3+post_trig)]) == trace_length: #Make sure the trace length is correct.\n",
    "            CH3.append(trace_3[(trigger_pt_3-pre_trig):(trigger_pt_3+post_trig)])\n",
    "            trigger_points_CH3.append(trigger_pt_3)\n",
    "    \n",
    "    return np.array(CH3)\n",
    "\n",
    "traces = get_trace_from_csv(filename, channel = 2,trace_length=1024, fs = 1/(0.02e-7),pre_trig = 512, post_trig = 512)\n",
    "#trace_length must match the noise trace length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "OFL_amplitude = []\n",
    "OFL_chi2 = []\n",
    "OFL_shift = []\n",
    "OF0_amplitude = []\n",
    "OF0_chi2 = []\n",
    "for data in traces:\n",
    "    fs = 5e9 # Scope sampling frequency\n",
    "    OF = pycrp.OptimalFilter(fs = Fs)\n",
    "    OFresult = OF.of_amp(S=data,N=noise[\"PSDs\"][\"CH1\"],T=pulse_average_normalize,fs=Fs,delay=25,\n",
    "                         LPF=1e8,LPF_OFP=10e8,interpolate=False,return_all=True,\n",
    "                         OFP_length=-1,OFP_threshold=0)\n",
    "    OF0_amplitude.append(OFresult[0][0])\n",
    "    OF0_chi2.append(OFresult[0][1])    \n",
    "    OFL_amplitude.append(OFresult[2][0])\n",
    "    OFL_chi2.append(OFresult[2][1])\n",
    "    OFL_shift.append(OFresult[1][2])\n",
    "    \n",
    "RQs = {\"OF0\":np.array(OF0_amplitude), \"OF0_chi2\":np.array(OF0_chi2),\n",
    "       \"OFL\":np.array(OFL_amplitude), \"OFL_chi2\":np.array(OFL_chi2),\"OFL_time\":np.array(OFL_shift), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (RQs[\"OFL_chi2\"] < 3)\n",
    "histgram = plt.hist(RQs[\"OFL\"][mask]*1e3,range = (-2, 16),bins = 200)\n",
    "plt.xlabel(\"OFL amplitude(mV)\")\n",
    "plt.ylabel(\"counts\")\n",
    "first_peak_fit = pycrp.Fit.fitGaus_from_hist(histgram[0], histgram[1], fitrange=(1,6), poissonerror=False,\n",
    "                  interp_points=100, p0=None, make_plot=True, sidebands=False, label = \"\")\n",
    "second_peak_fit = pycrp.Fit.fitGaus_from_hist(histgram[0], histgram[1], fitrange=(5,10), poissonerror=False,\n",
    "                  interp_points=100, p0=None, make_plot=True, sidebands=False, label = \"\")\n",
    "third_peak_fit = pycrp.Fit.fitGaus_from_hist(histgram[0], histgram[1], fitrange=(8,13), poissonerror=False,\n",
    "                  interp_points=100, p0=None, make_plot=True, sidebands=False, label = \"\")\n",
    "plt.title(\"Bandgap of Boardcom SiPM\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DAQ data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_channel(filename, noise_reject_rate = 0.01):\n",
    "    ch_avalible =[]\n",
    "    which_channels = []\n",
    "    for i in range(32):\n",
    "        ch_avalible.append(\"HIT_0_{0}\".format(i))\n",
    "        ch_avalible.append(\"HIT_1_{0}\".format(i))\n",
    "    hit_df = pd.read_csv(filename, sep=';', on_bad_lines='skip', usecols=ch_avalible).fillna(0).to_numpy()\n",
    "    ch_list = np.argwhere(np.sum(hit_df, axis = 0) > noise_reject_rate*len(hit_df))\n",
    "    which_channels = []\n",
    "    for index in ch_list:\n",
    "        if index[0] >= 32:\n",
    "            which_channels.append(index[0]-32)\n",
    "        else:\n",
    "            which_channels.append(index[0])\n",
    "    return which_channels\n",
    "\n",
    "def load_dataset(filename, which_channels = [\"0_8\",\"0_20\"], noise_reject_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Load DT5550W DAQ dataset (processing mode = \"FULL\" in the software)\n",
    "    \n",
    "    Inputs\n",
    "    -------\n",
    "    filename: str\n",
    "        Filename path\n",
    "    which_channels: list or \"auto\"\n",
    "        If which_channels is a list, read the sepcified channels\n",
    "        If which_channels is \"auto\", find active channels \n",
    "    noise_reject_rate: float in [0,1]\n",
    "        The percentage of events needed to be identified as active channels, required only when which_channels is \"auto\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe\n",
    "    \"\"\"\n",
    "    cols_list = pd.read_csv(filename, nrows=0).columns.tolist()[0].split(sep=\";\")\n",
    "    #metadata_name = cols_list[:10]+cols_list[int(len(cols_list)/2+2):int(len(cols_list)/2+8)]\n",
    "    metadata_name = cols_list[:10]\n",
    "    dtype_list = [\"HIT_\",\"CHARGE_\", \"COARSE_\", \"FINE_\",\"RELATIVETIME_\"]\n",
    "    #dtype_list = [\"HIT_\",\"CHARGE_\", \"COARSE_\", \"FINE_\",]\n",
    "    if which_channels == \"auto\":\n",
    "        ch_hit =[]\n",
    "        for i in range(32):\n",
    "            ch_hit.append(\"HIT_0_{0}\".format(i))\n",
    "            ch_hit.append(\"HIT_1_{0}\".format(i))\n",
    "        hit_df = pd.read_csv(filename, sep=';', on_bad_lines='skip', usecols=ch_hit).fillna(0).to_numpy()\n",
    "        \n",
    "        ch_list = np.argwhere(np.sum(hit_df, axis = 0) > noise_reject_rate*len(hit_df))\n",
    "        which_channels = []\n",
    "        for index in ch_list:\n",
    "            if index >= 32:\n",
    "                which_channels.append(\"1_{0}\".format(index[0]-32))\n",
    "            else:\n",
    "                which_channels.append(\"0_{0}\".format(index[0]))\n",
    "    data_ch = []\n",
    "    for channels in which_channels:\n",
    "        for ch_name in dtype_list:\n",
    "            data_ch.append(ch_name+channels)\n",
    "    return pd.read_csv(filename, sep=';', on_bad_lines='skip', usecols=metadata_name+data_ch)\n",
    "\n",
    "\n",
    "def find_alpha(df, channels = ['FINE_0_8', 'FINE_0_26']):\n",
    "    \"\"\"\n",
    "    FInd the calibration parameters for channels\n",
    "    \n",
    "    Inputs\n",
    "    -------\n",
    "    df: Dict\n",
    "        A dictionary from Pandas dataframe for DT5550W DAQ dataset\n",
    "\n",
    "    channels: list\n",
    "        Channels to calibrate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    alpha: list\n",
    "        calibration constent for channels\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = np.zeros(len(channels))\n",
    "    for i,ch in enumerate(channels):\n",
    "        mask = (np.array(df[ch]) > 4)& (np.array(df[ch]) < 1020) #remove fine time = 4, 1020 events\n",
    "        #mask = (np.array(df[ch]) > 200)& (np.array(df[ch]) < 1020) \n",
    "        print(\"Fine time TAC min:\", np.min(np.array(df[ch])[mask]))\n",
    "        print(\"Fine time TAC max:\", np.max(np.array(df[ch])[mask]))\n",
    "        alpha[i] = 25/(np.max(np.array(df[ch])[mask])-np.min(np.array(df[ch])[mask]))\n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger(filename, trigger_ch = \"0_4\"):\n",
    "    \"\"\"\n",
    "    Use one channel as the trigger channel\n",
    "    \"\"\"\n",
    "    df = load_dataset(filename, which_channels = \"auto\", noise_reject_rate = 0.05)\n",
    "    trigger_ch = 'HIT_'+trigger_ch\n",
    "    return df.where((df[trigger_ch] == 1)&(df[ch] == 1)).dropna()\n",
    "\n",
    "#Example:\n",
    "#trigger(filename, trigger_ch = \"0_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example for ToF related plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch1 = 4\n",
    "# ch2 = 26\n",
    "\n",
    "#plt.figure(figsize=(12, 8), dpi=100)\n",
    "#txt = [\"16*2 ns LEMO on CH26\", \"16ns LEMO on CH26\", \"No LEMO delay\", \"16ns LEMO on CH4\",\"16*2 ns LEMO on CH4\",\"16 ns LEMO on CH4\",\"No LEMO delay\"]\n",
    "\n",
    "for i in range(4):\n",
    "    filename = \"0330/{0}.data\".format(i+1)\n",
    "    try:\n",
    "        ch1, ch2 = find_channel(filename, noise_reject_rate = 0.15)[-2:]\n",
    "        df = load_dataset(filename, which_channels = \"auto\", noise_reject_rate = 0.15).to_dict('list')\n",
    "        print(df.keys())\n",
    "\n",
    "        #remove fine time = 4, 1020 events\n",
    "        mask = ((np.array(df['FINE_0_{0}'.format(ch1)]) > 4)&(np.array(df['FINE_0_{0}'.format(ch1)]) < 1020)&\n",
    "                (np.array(df['FINE_0_{0}'.format(ch2)]) > 4)&(np.array(df['FINE_0_{0}'.format(ch2)]) < 1020)) \n",
    "        \n",
    "        print(\"Number of hits on the 1st ch:\",np.array(df['HIT_0_{0}'.format(ch1)]).sum())\n",
    "        print(\"Number of hits on the 2nd ch:\",np.array(df['HIT_0_{0}'.format(ch2)]).sum())\n",
    "\n",
    "        alpha = find_alpha(df, channels = ['FINE_0_{0}'.format(ch1), 'FINE_0_{0}'.format(ch2)])\n",
    "        fine_time = np.multiply(alpha,np.array([df['FINE_0_{0}'.format(ch1)]-np.min(np.array(df['FINE_0_{0}'.format(ch1)])[mask]),\n",
    "                                                df['FINE_0_{0}'.format(ch2)]-np.min(np.array(df['FINE_0_{0}'.format(ch2)])[mask])]).T)\n",
    "        #print(fine_time)\n",
    "        coarse_time = np.array([df['COARSE_0_{0}'.format(ch1)],df['COARSE_0_{0}'.format(ch2)]])*25\n",
    "\n",
    "        time = (coarse_time-fine_time.T)\n",
    "        ToF = time[0]-time[1]\n",
    "\n",
    "        plt.figure(figsize=(12, 8), dpi=100)\n",
    "\n",
    "        # plot fine time\n",
    "        \n",
    "#         plt.hist(fine_time.T[0],bins = 400, range = (-1,26), histtype = \"step\")\n",
    "#         plt.xlabel(\"Fine time(ns) ASIC-A CH{0}\".format(ch1))\n",
    "#         plt.ylabel(\"counts\")\n",
    "#         plt.title(\"Fine time of one channel\")\n",
    "        \n",
    "        # plot fine time 2d hist\n",
    "\n",
    "#         plt.figure(figsize=(12, 8), dpi=100)\n",
    "#         plt.hist2d(ToF, fine_time.T[1] ,bins=(200,50), range=[[-20,60],[-1,26]])\n",
    "#         plt.xlabel(\"ToF(ns) CH8 - CH18\")\n",
    "#         plt.ylabel(\"Fine time(ns) CH18\")\n",
    "        \n",
    "        # plot ToF\n",
    "\n",
    "        center = np.abs(np.median(ToF))\n",
    "        \n",
    "        print(\"number of events:\",count(ToF, center))\n",
    "        \n",
    "        hist1 = plt.hist(np.abs(ToF),bins = 50, range = (center-10,center+10), histtype = \"step\")\n",
    "        fit = pycrp.Fit()\n",
    "        #fit_1 = fit.fitGaus_from_hist(hist1[0], hist1[1], interp_points=200, make_plot=True,label = \"{0}\".format(txt[i]))\n",
    "        fit_1 = fit.fitGaus_from_hist(hist1[0], hist1[1], interp_points=200, make_plot=True)\n",
    "        print(fit_1[0][1], fit_1[0][2])\n",
    "        plt.xlabel(\"ToF(ns) CH{0} - CH{1}\".format(ch1,ch2))\n",
    "        plt.ylabel(\"counts\")\n",
    "        #plt.title(\"ToF measurement with different channels on ASIC_B\")\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(find_channel(filename, noise_reject_rate = 0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0323/2.data\"\n",
    "load_dataset(filename, which_channels = [\"0_5\",\"0_19\"], noise_reject_rate = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 Kernel",
   "language": "python",
   "name": "ricochet.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
